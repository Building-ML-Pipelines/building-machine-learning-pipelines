{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beam Summit Workshop - Creating Custom TFX Components.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVXqEoG5LkjJ",
        "colab_type": "text"
      },
      "source": [
        "## Copy your notebook version\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIkC2_9RxtTi",
        "colab_type": "text"
      },
      "source": [
        "# Workshop - Developing TensorFlow Extended Components\n",
        "\n",
        "TLDR: TensorFlow Extended (TFX) allows data scientists to assemble production pipelines for model updates and then run the pipelines of a variety of orchestration tools.\n",
        "\n",
        "TFX provide basic components to ingest, validate and transform data, as well as for model training, tuning, validation and deployments. \n",
        "\n",
        "![TFX Pipeline](https://drive.google.com/uc?export=view&id=1yOPZTcIgF6arI7CLeWR1gTxrL1_MfQpL)\n",
        "\n",
        "Figure taken from \"Building Machine Learning Pipelines\", O'Reilly July 2020, Hapke, Nelson\n",
        "\n",
        "One of the strengths of TFX is the extensibility of the framework by building custom components.\n",
        "\n",
        "### Applications for custom components can:\n",
        "\n",
        "* Ingestion of user specific data (e.g. images or custom database tables)\n",
        "* Compiling specific pipeline reports\n",
        "* Communicating pipeline results (e.g. via Slack or MS Teams)\n",
        "* Generating additional pipeline artifacts, e.g. model and data cards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20edwgX_Q5Gl",
        "colab_type": "text"
      },
      "source": [
        "## Workshop Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj5qUAi-Q7q0",
        "colab_type": "text"
      },
      "source": [
        "In this workshop, we'll introduce two ways of building your TFX components for your ML pipelines. In particular, we'll focus on:\n",
        "\n",
        "* Brief overview of TFX and pipelines\n",
        "* Presentation how to build a component from scratch\n",
        "* Workshop how to extend existing components \n",
        "\n",
        "In this workshop we are implementing a TFX component to ingest images directly into the ML pipeline and generate labels for each image instead of converting the images to TFRecord representations outside of the pipeline.\n",
        "\n",
        "What are the benefits of the implementation?\n",
        "\n",
        "* Conversion is tracked in the ML Metadata store\n",
        "* Component output can be cached\n",
        "* No \"glue code\" required to connect the images to the pipeline\n",
        "\n",
        "![TFX **Component**](https://drive.google.com/uc?export=view&id=1zyPDzXH7V-wY-AHcmOsUGpgkyPYYm2iG)\n",
        "\n",
        "Figure taken from \"Building Machine Learning Pipelines\", O'Reilly July 2020, Hapke, Nelson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9-KiDB6QjZZ",
        "colab_type": "text"
      },
      "source": [
        "## TFX - Quick Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMRQ2TWuQpfj",
        "colab_type": "text"
      },
      "source": [
        "TFX provides a variety of stand-alone tools and pipeline components.\n",
        "\n",
        "\n",
        "![TFX Components](https://drive.google.com/uc?export=view&id=15ftktZN2o1MBim4sN29GTRW3hoUWzwfR)\n",
        "Figure taken from \"Building Machine Learning Pipelines\", O'Reilly July 2020, Hapke, Nelson\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5e76kJGQro4",
        "colab_type": "text"
      },
      "source": [
        "## TFX Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqeLOVLlQvsc",
        "colab_type": "text"
      },
      "source": [
        "TFX components consists of 3 parts: \n",
        "\n",
        "*   Component driver\n",
        "*   Component executor\n",
        "*   Component publisher\n",
        "\n",
        "The driver and publisher communicate with the ML metadata store and they retrieve the ML artifacts. Components pass data references from component to component and not the actual data!\n",
        "\n",
        "The action happens in the component executor. More later about that ...\n",
        "\n",
        "![TFX **Component**](https://drive.google.com/uc?export=view&id=1TopCi6XjouwJyVho0PEDIZl9fvEOA1UB)\n",
        "\n",
        "## How to implement a component?\n",
        "\n",
        "![TFX **Component** Implementation Details](https://drive.google.com/uc?export=view&id=1nDYwHpNFyY3r2GrylD8qiuPE2xM60RHI)\n",
        "\n",
        "Figures taken from \"Building Machine Learning Pipelines\", O'Reilly July 2020, Hapke, Nelson\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7bkQIAVQxH8",
        "colab_type": "text"
      },
      "source": [
        "## Extending existing TFX components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_JgTpdhQ0p4",
        "colab_type": "text"
      },
      "source": [
        "![TFX **Component**](https://drive.google.com/uc?export=view&id=1Hg-iUp8UF5Jh3dpdL-htG-Cw5g7GKqF3)\n",
        "\n",
        "### Benefits:\n",
        "\n",
        "* Less boiler plate code\n",
        "* Reuse of existing component drivers and publishers\n",
        "* Faster implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oqVb65hJEF5",
        "colab_type": "text"
      },
      "source": [
        "## Where to find more details?\n",
        "\n",
        "If you are interested in a detailed introduction to TensorFlow Extended and other TensorFlow libraries, check out the recent O'Reilly publication on machine learning pipelines.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17Rtpso9UrE6HmhxCmtyd0aETr3WKSZ0e\" width=\"450\">\n",
        "\n",
        "* [Amazon.com](https://www.amazon.com/dp/1492053198/)\n",
        "* [Powells.com](https://www.powells.com/book/building-machine-learning-pipelines-9781492053194)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD0G9msgQ3XX",
        "colab_type": "text"
      },
      "source": [
        "## Code Outline\n",
        "\n",
        "* Download example dataset\n",
        "* Install required Python packages\n",
        "* Restart notebook kernel\n",
        "* Import required packages & modules\n",
        "* Define helper functions\n",
        "* Walk through a component implementation from scratch\n",
        "* Implement a component by overwriting the component executor\n",
        "* Create a pipeline with the component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rxGlI475jTh",
        "colab_type": "text"
      },
      "source": [
        "## Download example dataset\n",
        "\n",
        "For this workshop we'll be using the public cats & dogs dataset created by Microsoft. The data set contains two folders: \"Dog\" and \"Cat\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-buPbpyo5lgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf /content/PetImages/\n",
        "!rm *.zip\n",
        "\n",
        "!wget https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n",
        "!unzip -q -d /content/ /content/kagglecatsanddogs_3367a.zip\n",
        "\n",
        "# number of images before reduction\n",
        "!echo \"Count images\"\n",
        "!ls -U /content/PetImages/Cat | wc -l\n",
        "!ls -U /content/PetImages/Dog | wc -l\n",
        "\n",
        "# remove lines below to test the pipeline with the entire dataset\n",
        "!echo \"Reduce images for demo purposes\"\n",
        "!cd /content/PetImages/Cat && ls -U | head -12000 | xargs rm \n",
        "!cd /content/PetImages/Dog && ls -U | head -12000 | xargs rm \n",
        "\n",
        "!echo \"Count images after removal\"\n",
        "!ls -U /content/PetImages/Cat | wc -l\n",
        "!ls -U /content/PetImages/Dog | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSOJwdKVPuR7",
        "colab_type": "text"
      },
      "source": [
        "## Install required Python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_HLg5-j5v29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!pip install -qU tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79kKPWzKPzKe",
        "colab_type": "text"
      },
      "source": [
        "## Restart notebook kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVjaCJ6PP1lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWVmbxJJI_M-",
        "colab_type": "text"
      },
      "source": [
        "## Import required packages & modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAlDXeya7f-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "from typing import Any, Dict, Iterable, List, Text\n",
        "\n",
        "import absl\n",
        "import apache_beam as beam\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tfx\n",
        "from google.protobuf import json_format\n",
        "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
        "from tensorflow_transform.saved import saved_transform_io\n",
        "from tensorflow_transform.tf_metadata import (dataset_metadata, dataset_schema,\n",
        "                                              metadata_io, schema_utils)\n",
        "from tfx import types\n",
        "from tfx.components import (Evaluator, Pusher, ResolverNode, StatisticsGen,\n",
        "                            Trainer)\n",
        "from tfx.components.base import (base_component, base_driver, base_executor,\n",
        "                                 executor_spec)\n",
        "from tfx.components.example_gen import driver\n",
        "from tfx.components.example_gen.base_example_gen_executor import (\n",
        "    INPUT_KEY, BaseExampleGenExecutor)\n",
        "from tfx.components.example_gen.component import FileBasedExampleGen\n",
        "from tfx.components.example_gen.import_example_gen.component import \\\n",
        "    ImportExampleGen\n",
        "from tfx.components.example_gen.utils import dict_to_example\n",
        "from tfx.components.example_validator.component import ExampleValidator\n",
        "from tfx.components.schema_gen.component import SchemaGen\n",
        "from tfx.components.statistics_gen.component import StatisticsGen\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.components.transform.component import Transform\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import data_types, metadata, pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import \\\n",
        "    InteractiveContext\n",
        "from tfx.proto import evaluator_pb2, example_gen_pb2, pusher_pb2, trainer_pb2\n",
        "from tfx.types import (Channel, artifact_utils, channel_utils,\n",
        "                       standard_artifacts)\n",
        "from tfx.types.component_spec import ChannelParameter, ExecutionParameter\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
        "from tfx.utils import io_utils\n",
        "from tfx.utils.dsl_utils import external_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk30idMCd-Jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd9XyDkBQJBx",
        "colab_type": "text"
      },
      "source": [
        "## Define helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrKzooGT8Km0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _int64_feature(value):\n",
        "    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
        "    if not isinstance(value, list):\n",
        "        value = [value]\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def get_label_from_filename(filename):\n",
        "    \"\"\" Function to set the label for each image. In our case, we'll use the file \n",
        "    path of a label indicator. Based on your initial data \n",
        "    Args:\n",
        "      filename: string, full file path\n",
        "    Returns:\n",
        "      int - label\n",
        "    Raises:\n",
        "      NotImplementedError if not label category was detected\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    lowered_filename = filename.lower()\n",
        "    if \"dog\" in lowered_filename:\n",
        "        label = 0\n",
        "    elif \"cat\" in lowered_filename:\n",
        "        label = 1\n",
        "    else:\n",
        "        raise NotImplementedError(\"Found unknown image\")\n",
        "    return label\n",
        "    \n",
        "\n",
        "def _convert_to_example(image_buffer, label):\n",
        "    \"\"\"Function to convert image byte strings and labels into tf.Example structures\n",
        "      Args:\n",
        "        image_buffer: byte string representing the image\n",
        "        label: int\n",
        "      Returns:\n",
        "        TFExample data structure containing the image (byte string) and the label (int encoded)\n",
        "    \"\"\"\n",
        "\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature={\n",
        "                'image/raw': _bytes_feature(image_buffer),\n",
        "                'label': _int64_feature(label)\n",
        "            }))\n",
        "    return example\n",
        "\n",
        "\n",
        "def get_image_data(filename):\n",
        "    \"\"\"Process a single image file.\n",
        "    Args:\n",
        "      filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
        "    Returns:\n",
        "      TFExample data structure containing the image (byte string) and the label (int encoded)\n",
        "    \"\"\"\n",
        "    label = get_label_from_filename(filename)\n",
        "    byte_content = tf.io.read_file(filename)\n",
        "    rs = _convert_to_example(byte_content.numpy(), label)\n",
        "    return rs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWyopU1Q-5c2",
        "colab_type": "text"
      },
      "source": [
        "## Walk through a component implementation from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHn3GHAm--B0",
        "colab_type": "text"
      },
      "source": [
        "### Custom Component Specifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjZ_QvArnl1",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/tensorflow/tfx/blob/master/tfx/types/standard_artifacts.py\n",
        "\n",
        "Difference between ChannelParameter and ExecutionParameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmxjmr4p-1nW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomIngestionComponentSpec(types.ComponentSpec):\n",
        "    \"\"\"ComponentSpec for Custom Ingestion Component.\"\"\"\n",
        "    \n",
        "    PARAMETERS = {\n",
        "        'name': ExecutionParameter(type=Text),\n",
        "    }\n",
        "    INPUTS = {\n",
        "        'input': ChannelParameter(type=standard_artifacts.ExternalArtifact),\n",
        "    }\n",
        "    OUTPUTS = {\n",
        "        'examples': ChannelParameter(type=standard_artifacts.Examples),\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvpcpzhB_JO-",
        "colab_type": "text"
      },
      "source": [
        "### Custom Component Executor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FEMWVdP_GhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomIngestionExecutor(base_executor.BaseExecutor):\n",
        "    \"\"\"Executor for CustomIngestionComponent.\"\"\"\n",
        "\n",
        "    def Do(self, input_dict: Dict[Text, List[types.Artifact]],\n",
        "          output_dict: Dict[Text, List[types.Artifact]],\n",
        "          exec_properties: Dict[Text, Any]) -> None:\n",
        "\n",
        "        input_base_uri = artifact_utils.get_single_uri(input_dict['input'])\n",
        "        image_files = tf.io.gfile.listdir(input_base_uri)\n",
        "        random.shuffle(image_files)\n",
        "\n",
        "        train_images, eval_images = image_files[10000:], image_files[:10000]\n",
        "        splits = [('train', train_images), ('eval', eval_images)]\n",
        "\n",
        "        for split_name, images in splits:\n",
        "            output_dir = artifact_utils.get_split_uri(\n",
        "                output_dict['examples'], split_name)\n",
        "            \n",
        "            tfrecords_filename = os.path.join(output_dir, 'images.tfrecords')\n",
        "            \n",
        "            options = tf.io.TFRecordOptions(compression_type=None)\n",
        "            writer = tf.io.TFRecordWriter(tfrecords_filename, options=options)\n",
        "\n",
        "            for image_filename in images:\n",
        "                image_path = os.path.join(input_base_uri, image_filename)\n",
        "                example = get_image_data(image_path)\n",
        "                writer.write(example.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh5vr3Pa_VpD",
        "colab_type": "text"
      },
      "source": [
        "### Custom Component Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjPrxM5X_Rdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomIngestionDriver(base_driver.BaseDriver):\n",
        "    \"\"\"Custom driver for CustomIngestion component.\n",
        "\n",
        "    This driver supports file based ExampleGen, it registers external file path as\n",
        "    an artifact, similar to the use cases CsvExampleGen and ImportExampleGen.\n",
        "    \"\"\"\n",
        "\n",
        "    def resolve_input_artifacts(\n",
        "        self,\n",
        "        input_channels: Dict[Text, types.Channel],\n",
        "        exec_properties: Dict[Text, Any],\n",
        "        driver_args: data_types.DriverArgs,\n",
        "        pipeline_info: data_types.PipelineInfo,\n",
        "    ) -> Dict[Text, List[types.Artifact]]:\n",
        "        \"\"\"Overrides BaseDriver.resolve_input_artifacts().\"\"\"\n",
        "        del driver_args  # unused\n",
        "        del pipeline_info  # unused\n",
        "\n",
        "        input_config = example_gen_pb2.Input()\n",
        "        input_dict = channel_utils.unwrap_channel_dict(input_channels)\n",
        "        for input_list in input_dict.values():\n",
        "            for single_input in input_list:\n",
        "                self._metadata_handler.publish_artifacts([single_input])\n",
        "                \n",
        "        return input_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iKbYwCE_bZR",
        "colab_type": "text"
      },
      "source": [
        "### Component Component \n",
        "\n",
        "Putting all pieces together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qooyPE9X_agc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomIngestionComponent(base_component.BaseComponent):\n",
        "    \"\"\"CustomIngestion Component.\"\"\"\n",
        "    SPEC_CLASS = CustomIngestionComponentSpec\n",
        "    EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(CustomIngestionExecutor)\n",
        "    DRIVER_CLASS = CustomIngestionDriver\n",
        "\n",
        "    def __init__(self,\n",
        "                input: types.Channel = None,\n",
        "                output_data: types.Channel = None,\n",
        "                name: Text = None):\n",
        "      if not output_data:\n",
        "          examples_artifact = standard_artifacts.Examples()\n",
        "          examples_artifact.split_names = artifact_utils.encode_split_names(['train', 'eval'])\n",
        "          output_data = channel_utils.as_channel([examples_artifact])\n",
        "      spec = CustomIngestionComponentSpec(input=input,\n",
        "                                          examples=output_data, \n",
        "                                          name=name)\n",
        "      super(CustomIngestionComponent, self).__init__(spec=spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPsN9SQN_muf",
        "colab_type": "text"
      },
      "source": [
        "## Basic Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiPWvcY4_g_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_context = InteractiveContext()\n",
        "\n",
        "data_root = os.path.join(\"/content/\", 'PetImages', 'Dog')\n",
        "examples = external_input(data_root)\n",
        "\n",
        "ingest_images = CustomIngestionComponent(\n",
        "    input=examples, name='ImageIngestionComponent')\n",
        "test_context.run(ingest_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUL5vJh0_uyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=ingest_images.outputs['examples'])\n",
        "test_context.run(statistics_gen)\n",
        "\n",
        "test_context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Go6dW3AeQpM",
        "colab_type": "text"
      },
      "source": [
        "## Implement a component by overwriting the component executor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNVTLErTx7B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@beam.ptransform_fn \n",
        "def ImageToExample(\n",
        "      pipeline: beam.Pipeline,\n",
        "      input_dict: Dict[Text, List[types.Artifact]],\n",
        "      exec_properties: Dict[Text, Any],\n",
        "      split_pattern: Text) -> beam.pvalue.PCollection:\n",
        "    \"\"\"Read jpeg files and transform to TF examples.\n",
        "\n",
        "    Note that each input split will be transformed by this function separately.\n",
        "\n",
        "    Args:\n",
        "        pipeline: beam pipeline.\n",
        "        input_dict: Input dict from input key to a list of Artifacts.\n",
        "          - input_base: input dir that contains the image data.\n",
        "        exec_properties: A dict of execution properties.\n",
        "        split_pattern: Split.pattern in Input config, glob relative file pattern\n",
        "          that maps to input files with root directory given by input_base.\n",
        "\n",
        "    Returns:\n",
        "        PCollection of TF examples.\n",
        "    \"\"\"\n",
        "\n",
        "    input_base_uri = artifact_utils.get_single_uri(input_dict['input'])\n",
        "    image_pattern = os.path.join(input_base_uri, split_pattern)\n",
        "    absl.logging.info(\n",
        "        'Processing input image data {} to TFExample.'.format(image_pattern))\n",
        "\n",
        "    image_files = tf.io.gfile.glob(image_pattern)\n",
        "    if not image_files:\n",
        "        raise RuntimeError(\n",
        "            'Split pattern {} does not match any files.'.format(image_pattern))\n",
        "\n",
        "    return (\n",
        "        pipeline\n",
        "        | beam.Create(image_files)\n",
        "        | 'ConvertImagesToBase64' >> beam.Map(lambda file: get_image_data(file))\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwk_iZXdyA8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageExampleGenExecutor(BaseExampleGenExecutor):\n",
        "    \"\"\"TFX example gen executor for processing jpeg format.\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "      from tfx.components.example_gen.component import\n",
        "      FileBasedExampleGen\n",
        "      from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "      example_gen = FileBasedExampleGen(\n",
        "          input=external_input(\"/content/PetImages/\"),\n",
        "          input_config=input_config,\n",
        "          output_config=output,\n",
        "          custom_executor_spec=executor_spec.ExecutorClassSpec(_Executor))\n",
        "    \"\"\"\n",
        "\n",
        "    def GetInputSourceToExamplePTransform(self) -> beam.PTransform:\n",
        "        \"\"\"Returns PTransform for image to TF examples.\"\"\"\n",
        "        return ImageToExample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzWHV-LbesyZ",
        "colab_type": "text"
      },
      "source": [
        "## Building your ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpUFbbLzyHpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context = InteractiveContext()\n",
        "\n",
        "output = example_gen_pb2.Output(\n",
        "             split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "                 example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=4),\n",
        "                 example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
        "             ]))\n",
        "\n",
        "input_config = example_gen_pb2.Input(splits=[\n",
        "    example_gen_pb2.Input.Split(name='images', pattern='*/*.jpg'),\n",
        "])\n",
        "\n",
        "example_gen = FileBasedExampleGen(\n",
        "    input=external_input(\"/content/PetImages/\"),\n",
        "    input_config=input_config,\n",
        "    output_config=output,\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(ImageExampleGenExecutor))\n",
        "\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5acSHe-yQaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vg7UVCi8f9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsqm7TUsTrrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=True)\n",
        "\n",
        "context.run(schema_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIbnThIOHsXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHear9cQU-yM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "\n",
        "context.run(example_validator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C19JFcpVNHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile constants.py\n",
        "\n",
        "from typing import Text\n",
        "\n",
        "def transformed_name(key: Text) -> Text:\n",
        "  \"\"\"Generate the name of the transformed feature from original name.\"\"\"\n",
        "  return key + '_xf'\n",
        "\n",
        "# Keys\n",
        "LABEL_KEY = 'label'\n",
        "INPUT_KEY = 'image/raw'\n",
        "\n",
        "# Feature keys\n",
        "RAW_FEATURE_KEYS = [INPUT_KEY]\n",
        "\n",
        "# Constants\n",
        "IMG_SIZE = 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3V04th4YEV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile transform.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import logging\n",
        "\n",
        "from typing import Union, Dict\n",
        "\n",
        "import constants\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def convert_image(raw_image: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "    if tf.io.is_jpeg(raw_image):\n",
        "        image = tf.io.decode_jpeg(raw_image, channels=3)\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        image = (image / 127.5) - 1 \n",
        "        image = tf.image.resize(image, [constants.IMG_SIZE, constants.IMG_SIZE])\n",
        "        return image\n",
        "    return tf.constant(np.zeros((constants.IMG_SIZE, constants.IMG_SIZE, 3)), tf.float32)\n",
        "\n",
        "def fill_in_missing(x: Union[tf.Tensor, tf.SparseTensor]) -> tf.Tensor:\n",
        "    \"\"\"Replace missing values in a SparseTensor.\n",
        "\n",
        "    Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
        "\n",
        "    Args:\n",
        "      x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "        in the second dimension.\n",
        "\n",
        "    Returns:\n",
        "      A rank 1 tensor where missing values of `x` have been filled in.\n",
        "    \"\"\"\n",
        "    if isinstance(x, tf.sparse.SparseTensor):\n",
        "        default_value = \"\" if x.dtype == tf.string else 0\n",
        "        x = tf.sparse.to_dense(\n",
        "            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "            default_value,\n",
        "        )\n",
        "    return tf.squeeze(x, axis=1)\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs: Dict[str, Union[tf.Tensor, tf.SparseTensor]]) -> Dict[str, tf.Tensor]:\n",
        "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "\n",
        "    for key in constants.RAW_FEATURE_KEYS:\n",
        "        image = fill_in_missing(inputs[key])\n",
        "        outputs[constants.transformed_name(key)] = convert_image(images)\n",
        "    \n",
        "    outputs[constants.transformed_name(constants.LABEL_KEY)] = inputs[constants.LABEL_KEY]\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnCPhVR6YZM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(\"transform.py\"))\n",
        "\n",
        "context.run(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5xD809NYkVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {\"trainer.py\"}\n",
        "\n",
        "from typing import List, Text, Dict\n",
        "\n",
        "import os\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from datetime import datetime\n",
        "\n",
        "from tfx.components.trainer.executor import TrainerFnArgs\n",
        "\n",
        "import constants\n",
        "\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
        "\n",
        "    return tf.data.TFRecordDataset(\n",
        "        filenames,\n",
        "        compression_type='GZIP')\n",
        "  \n",
        "\n",
        "def _get_label_for_image(model, tf_transform_output):\n",
        "    \"\"\"Returns a function that parses a raw byte image and applies TFT.\"\"\"\n",
        "\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "      \n",
        "    @tf.function\n",
        "    def serve_images_fn(image_raw):\n",
        "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "\n",
        "        image_raw = tf.reshape(image_raw, [-1, 1])\n",
        "        parsed_features = {'image': image_raw}\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        return model(transformed_features)\n",
        "\n",
        "    return serve_images_fn\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
        "\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "        \n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        feature_spec.pop(constants.LABEL_KEY)\n",
        "\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        return model(transformed_features)\n",
        "\n",
        "    return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text], \n",
        "              tf_transform_output: tft.TFTransformOutput, \n",
        "              batch_size: int = 32, \n",
        "              is_train: bool = False) -> tf.data.Dataset:\n",
        "    \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "    Args:\n",
        "      file_pattern: input tfrecord file pattern.\n",
        "      tf_transform_output: A TFTransformOutput.\n",
        "      batch_size: representing the number of consecutive elements of returned\n",
        "        dataset to combine in a single batch\n",
        "\n",
        "    Returns:\n",
        "      A dataset that contains (features, indices) tuple where features is a\n",
        "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "    \"\"\"\n",
        "    transformed_feature_spec = (\n",
        "        tf_transform_output.transformed_feature_spec().copy())\n",
        "\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transformed_feature_spec,\n",
        "        reader=_gzip_reader_fn,\n",
        "        label_key=constants.transformed_name(constants.LABEL_KEY))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_model() -> tf.keras.Model:\n",
        "    \"\"\"Creates a CNN Keras model based on transfer learning for classifying image data.\n",
        "\n",
        "    Returns:\n",
        "      A keras Model.\n",
        "    \"\"\"\n",
        "    img_shape = (constants.IMG_SIZE, constants.IMG_SIZE, 3)\n",
        "\n",
        "    # Create the base model from the pre-trained model MobileNet V2\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=img_shape,\n",
        "                                                    include_top=False,\n",
        "                                                    weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "    base_model.summary()\n",
        "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "      \n",
        "    output = tf.keras.layers.Dense(1)\n",
        "      \n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=img_shape, name=constants.transformed_name(constants.INPUT_KEY)),\n",
        "        base_model,\n",
        "        global_average_layer,\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        output\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.optimizers.RMSprop(lr=0.01),\n",
        "        loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=[tf.metrics.BinaryAccuracy(name='accuracy')])\n",
        "    model.summary()\n",
        "      \n",
        "    return model\n",
        "\n",
        "\n",
        "def run_fn(fn_args: TrainerFnArgs):\n",
        "    \"\"\"Train the model based on given args.\n",
        "\n",
        "    Args:\n",
        "      fn_args: Holds args used to train the model as name/value pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "    train_dataset = _input_fn(fn_args.train_files, tf_transform_output,\n",
        "                              TRAIN_BATCH_SIZE, is_train = True)\n",
        "    eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output,\n",
        "                             EVAL_BATCH_SIZE)\n",
        "\n",
        "    # check for availabe tpu and gpu units\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = get_model()\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "    )\n",
        "\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "            _get_serve_tf_examples_fn(model,\n",
        "                                      tf_transform_output).get_concrete_function(\n",
        "                                          tf.TensorSpec(\n",
        "                                              shape=[None],\n",
        "                                              dtype=tf.string,\n",
        "                                              name='examples')),\n",
        "\n",
        "    }\n",
        "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9oWjqJ6eFyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(\n",
        "    module_file=os.path.abspath(\"trainer.py\"),\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=1600),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=200))\n",
        "\n",
        "context.run(trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJgCzXERexin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[\n",
        "        tfma.ModelSpec(label_key='label')\n",
        "    ],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(\n",
        "            metrics=[\n",
        "                tfma.MetricConfig(class_name='ExampleCount'),\n",
        "                tfma.MetricConfig(class_name='AUC'),\n",
        "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
        "                  threshold=tfma.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          lower_bound={'value': 0.65}),\n",
        "                      change_threshold=tfma.GenericChangeThreshold(\n",
        "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                          absolute={'value': 0.01})))\n",
        "            ]\n",
        "        )\n",
        "    ],\n",
        "    slicing_specs=[\n",
        "        tfma.SlicingSpec()\n",
        "    ])\n",
        "\n",
        "model_resolver = ResolverNode(\n",
        "      instance_name='latest_blessed_model_resolver',\n",
        "      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
        "      model=Channel(type=Model),\n",
        "      model_blessing=Channel(type=ModelBlessing))\n",
        "\n",
        "context.run(model_resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo0OFYFnqfLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config)\n",
        "\n",
        "context.run(evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEIC8jk_q5wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context.show(evaluator.outputs['evaluation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUercE3rcJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_serving_model_dir = \"/content/exported_model\"\n",
        "\n",
        "pusher = Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    model_blessing=evaluator.outputs['blessing'],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "\n",
        "context.run(pusher)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x48NKMOEs2xI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Entire End-to-End Pipeline with Apache Beam\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrBOIMi2iXYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roGcqObUidSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "from typing import Any, Dict, Iterable, List, Text\n",
        "\n",
        "import absl\n",
        "import apache_beam as beam\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tfx\n",
        "from google.protobuf import json_format\n",
        "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
        "from tensorflow_transform.saved import saved_transform_io\n",
        "from tensorflow_transform.tf_metadata import (dataset_metadata, dataset_schema,\n",
        "                                              metadata_io, schema_utils)\n",
        "from tfx import types\n",
        "from tfx.components import (Evaluator, Pusher, ResolverNode, StatisticsGen,\n",
        "                            Trainer)\n",
        "from tfx.components.base import (base_component, base_driver, base_executor,\n",
        "                                 executor_spec)\n",
        "from tfx.components.example_gen import driver\n",
        "from tfx.components.example_gen.base_example_gen_executor import (\n",
        "    INPUT_KEY, BaseExampleGenExecutor)\n",
        "from tfx.components.example_gen.component import FileBasedExampleGen\n",
        "from tfx.components.example_gen.import_example_gen.component import \\\n",
        "    ImportExampleGen\n",
        "from tfx.components.example_gen.utils import dict_to_example\n",
        "from tfx.components.example_validator.component import ExampleValidator\n",
        "from tfx.components.schema_gen.component import SchemaGen\n",
        "from tfx.components.statistics_gen.component import StatisticsGen\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.components.transform.component import Transform\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import data_types, metadata, pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import \\\n",
        "    InteractiveContext\n",
        "from tfx.proto import evaluator_pb2, example_gen_pb2, pusher_pb2, trainer_pb2\n",
        "from tfx.types import (Channel, artifact_utils, channel_utils,\n",
        "                       standard_artifacts)\n",
        "from tfx.types.component_spec import ChannelParameter, ExecutionParameter\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
        "from tfx.utils import io_utils\n",
        "from tfx.utils.dsl_utils import external_input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OjqVdTAn-_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {\"component_helper.py\"}\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def _int64_feature(value):\n",
        "    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
        "    if not isinstance(value, list):\n",
        "        value = [value]\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def get_label_from_filename(filename):\n",
        "    \"\"\" Function to set the label for each image. In our case, we'll use the file \n",
        "    path of a label indicator. Based on your initial data \n",
        "    Args:\n",
        "      filename: string, full file path\n",
        "    Returns:\n",
        "      int - label\n",
        "    Raises:\n",
        "      NotImplementedError if not label category was detected\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    lowered_filename = filename.lower()\n",
        "    if \"dog\" in lowered_filename:\n",
        "        label = 0\n",
        "    elif \"cat\" in lowered_filename:\n",
        "        label = 1\n",
        "    else:\n",
        "        raise NotImplementedError(\"Found unknown image\")\n",
        "    return label\n",
        "    \n",
        "\n",
        "def _convert_to_example(image_buffer, label):\n",
        "    \"\"\"Function to convert image byte strings and labels into tf.Example structures\n",
        "      Args:\n",
        "        image_buffer: byte string representing the image\n",
        "        label: int\n",
        "      Returns:\n",
        "        TFExample data structure containing the image (byte string) and the label (int encoded)\n",
        "    \"\"\"\n",
        "\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature={\n",
        "                'image/raw': _bytes_feature(image_buffer),\n",
        "                'label': _int64_feature(label)\n",
        "            }))\n",
        "    return example\n",
        "\n",
        "\n",
        "def get_image_data(filename):\n",
        "    \"\"\"Process a single image file.\n",
        "    Args:\n",
        "      filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
        "    Returns:\n",
        "      TFExample data structure containing the image (byte string) and the label (int encoded)\n",
        "    \"\"\"\n",
        "    label = get_label_from_filename(filename)\n",
        "    byte_content = tf.io.read_file(filename)\n",
        "    rs = _convert_to_example(byte_content.numpy(), label)\n",
        "    return rs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4gyIoS1iwO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from component_helper import get_image_data\n",
        "\n",
        "@beam.ptransform_fn \n",
        "def ImageToExample(\n",
        "      pipeline: beam.Pipeline,\n",
        "      input_dict: Dict[Text, List[types.Artifact]],\n",
        "      exec_properties: Dict[Text, Any],\n",
        "      split_pattern: Text) -> beam.pvalue.PCollection:\n",
        "    \"\"\"Read jpeg files and transform to TF examples.\n",
        "\n",
        "    Note that each input split will be transformed by this function separately.\n",
        "\n",
        "    Args:\n",
        "        pipeline: beam pipeline.\n",
        "        input_dict: Input dict from input key to a list of Artifacts.\n",
        "          - input_base: input dir that contains the image data.\n",
        "        exec_properties: A dict of execution properties.\n",
        "        split_pattern: Split.pattern in Input config, glob relative file pattern\n",
        "          that maps to input files with root directory given by input_base.\n",
        "\n",
        "    Returns:\n",
        "        PCollection of TF examples.\n",
        "    \"\"\"\n",
        "\n",
        "    input_base_uri = artifact_utils.get_single_uri(input_dict['input'])\n",
        "    image_pattern = os.path.join(input_base_uri, split_pattern)\n",
        "    absl.logging.info(\n",
        "        'Processing input image data {} to TFExample.'.format(image_pattern))\n",
        "\n",
        "    image_files = tf.io.gfile.glob(image_pattern)\n",
        "    if not image_files:\n",
        "        raise RuntimeError(\n",
        "            'Split pattern {} does not match any files.'.format(image_pattern))\n",
        "\n",
        "    return (\n",
        "        pipeline\n",
        "        | beam.Create(image_files)\n",
        "        | 'ConvertImagesToBase64' >> beam.Map(lambda file: get_image_data(file))\n",
        "    )\n",
        "\n",
        "class ImageExampleGenExecutor(BaseExampleGenExecutor):\n",
        "    \"\"\"TFX example gen executor for processing jpeg format.\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "      from tfx.components.example_gen.component import\n",
        "      FileBasedExampleGen\n",
        "      from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "      example_gen = FileBasedExampleGen(\n",
        "          input=external_input(\"/content/PetImages/\"),\n",
        "          input_config=input_config,\n",
        "          output_config=output,\n",
        "          custom_executor_spec=executor_spec.ExecutorClassSpec(_Executor))\n",
        "    \"\"\"\n",
        "\n",
        "    def GetInputSourceToExamplePTransform(self) -> beam.PTransform:\n",
        "        \"\"\"Returns PTransform for image to TF examples.\"\"\"\n",
        "        return ImageToExample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYMaHKVaidbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = example_gen_pb2.Output(\n",
        "             split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "                 example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=4),\n",
        "                 example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
        "             ]))\n",
        "\n",
        "input_config = example_gen_pb2.Input(splits=[\n",
        "    example_gen_pb2.Input.Split(name='images', pattern='*/*.jpg'),\n",
        "])\n",
        "\n",
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[\n",
        "        tfma.ModelSpec(label_key='label')\n",
        "    ],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(\n",
        "            metrics=[\n",
        "                tfma.MetricConfig(class_name='ExampleCount'),\n",
        "                tfma.MetricConfig(class_name='AUC'),\n",
        "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
        "                  threshold=tfma.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          lower_bound={'value': 0.65}),\n",
        "                      change_threshold=tfma.GenericChangeThreshold(\n",
        "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                          absolute={'value': 0.01})))\n",
        "            ]\n",
        "        )\n",
        "    ],\n",
        "    slicing_specs=[\n",
        "        tfma.SlicingSpec()\n",
        "    ])\n",
        "\n",
        "_serving_model_dir = \"/content/exported_model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maCe2N9i6g-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_gen = FileBasedExampleGen(\n",
        "    input=external_input(\"/content/PetImages/\"),\n",
        "    input_config=input_config,\n",
        "    output_config=output,\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(ImageExampleGenExecutor))\n",
        "\n",
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=True)\n",
        "\n",
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(\"transform.py\"))\n",
        "\n",
        "trainer = Trainer(\n",
        "    module_file=os.path.abspath(\"trainer.py\"),\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=500),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=200))\n",
        "\n",
        "model_resolver = ResolverNode(\n",
        "      instance_name='latest_blessed_model_resolver',\n",
        "      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
        "      model=Channel(type=Model),\n",
        "      model_blessing=Channel(type=ModelBlessing))\n",
        "\n",
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config)\n",
        "\n",
        "pusher = Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    model_blessing=evaluator.outputs['blessing'],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO2hXNj-r9lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline_name = \"dogs_cats_pipeline\"\n",
        "\n",
        "# pipeline inputs\n",
        "base_dir = os.getcwd()\n",
        "pipeline_dir = os.path.join(base_dir, \"pipeline\")\n",
        "\n",
        "# pipeline outputs\n",
        "output_base = os.path.join(pipeline_dir, \"output\", pipeline_name)\n",
        "pipeline_root = os.path.join(output_base, \"pipeline_root\")\n",
        "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")\n",
        "\n",
        "\n",
        "def init_beam_pipeline(\n",
        "    components, pipeline_root: Text, direct_num_workers: int\n",
        ") -> pipeline.Pipeline:\n",
        "\n",
        "    absl.logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
        "    beam_arg = [\n",
        "        f\"--direct_num_workers={direct_num_workers}\",\n",
        "    ]\n",
        "\n",
        "    p = pipeline.Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        beam_pipeline_args=beam_arg,\n",
        "    )\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5robsp2l-It9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "components = [\n",
        "    example_gen,\n",
        "    statistics_gen,\n",
        "    schema_gen,\n",
        "    example_validator,\n",
        "    transform,\n",
        "    trainer,\n",
        "    model_resolver,\n",
        "    evaluator,\n",
        "    pusher,\n",
        "]\n",
        "\n",
        "p = init_beam_pipeline(components, pipeline_root, direct_num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IymHAJzWAWM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BeamDagRunner().run(p)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}