{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing your model with TensorFlow Model Analysis and the What-If Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB This only works in a Jupyter Notebook, NOT Jupyter Lab.\n",
    "Lab extensions have not been released for TFMA and the What-If Tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# stop tf warnings \n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need a trained model and an evaluation dataset (TFRecords) as produced by the earlier steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EVAL_DATA_FILE = 'data_tfrecord-00000-of-00001'\n",
    "_MODEL_DIR = 'serving_model_dir_2000_steps/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_shared_model = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path=_MODEL_DIR, tags=[tf.saved_model.SERVING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [tfma.slicer.SingleSliceSpec(),\n",
    "          tfma.slicer.SingleSliceSpec(columns=['product'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "        metrics_specs=[\n",
    "              tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                tfma.MetricConfig(class_name='FalsePositives'),\n",
    "                tfma.MetricConfig(class_name='TruePositives'),\n",
    "                tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "                tfma.MetricConfig(class_name='TrueNegatives')\n",
    "              ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model,\n",
    "    eval_config=eval_config,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_2000_steps\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take 2 goes\n",
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result, slicing_spec=slices[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_shared_model_2 = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path='serving_model_dir_150_steps/', tags=[tf.saved_model.SERVING])\n",
    "\n",
    "eval_result_2 = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model_2,\n",
    "    eval_config=eval_config,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_150_steps\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_from_disk = tfma.load_eval_results(\n",
    "    ['./eval_result_2000_steps','./eval_result_150_steps'], tfma.constants.MODEL_CENTRIC_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bug - only works reliably in Colab\n",
    "tfma.view.render_time_series(eval_results_from_disk, slices[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating against thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_threshold=tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "    slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "    metrics_specs=[\n",
    "          tfma.MetricsSpec(metrics=[\n",
    "              tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "              tfma.MetricConfig(class_name='ExampleCount'),\n",
    "              tfma.MetricConfig(class_name='AUC')\n",
    "              ],\n",
    "              thresholds={\n",
    "                  'AUC':\n",
    "                      tfma.config.MetricThreshold(\n",
    "                          value_threshold=tfma.GenericValueThreshold(\n",
    "                              lower_bound={'value': 0.5}))}\n",
    "                          )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_shared_models = [\n",
    "  tfma.default_eval_shared_model(\n",
    "      model_name='candidate', # must have this exact name\n",
    "      eval_saved_model_path='serving_model_dir_150_steps/', tags=[tf.saved_model.SERVING]),\n",
    "  tfma.default_eval_shared_model(\n",
    "      model_name='baseline', # must have this exact name\n",
    "      eval_saved_model_path='serving_model_dir_2000_steps/', tags=[tf.saved_model.SERVING]),\n",
    "]\n",
    "\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_models,\n",
    "    eval_config=eval_config_threshold,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_threshold\",slice_spec = slices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.load_validation_result('./eval_threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorboard/blob/master/docs/fairness-indicators.md\n",
    "# needs environment without WIT,but with TF2.x, TFX\n",
    "!pip install tensorboard_plugin_fairness_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_fairness=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "        metrics_specs=[\n",
    "              tfma.MetricsSpec(metrics=[\n",
    "                  tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                  tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                  tfma.MetricConfig(class_name='FalsePositives'),\n",
    "                  tfma.MetricConfig(class_name='TruePositives'),\n",
    "                  tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "                  tfma.MetricConfig(class_name='TrueNegatives'),\n",
    "                  tfma.MetricConfig(class_name='FairnessIndicators', config='{\"thresholds\":[0.25, 0.5, 0.75]}')\n",
    "              ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model,\n",
    "    eval_config=eval_config_fairness,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_fairness\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard_plugin_fairness_indicators import summary_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer('./fairness_indicator_logs')\n",
    "with writer.as_default():\n",
    "    summary_v2.FairnessIndicators('./eval_result_fairness', step=1)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./fairness_indicator_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The What-If Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from witwidget.notebook.visualization import WitConfigBuilder\n",
    "from witwidget.notebook.visualization import WitWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = tf.data.TFRecordDataset(_EVAL_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = [tf.train.Example.FromString(d.numpy()) for d in eval_data.take(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(export_dir=_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(examples):\n",
    "    preds = model.signatures['serving_default'](examples=tf.constant([example.SerializeToString() for example in examples]))\n",
    "    return preds['outputs'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = WitConfigBuilder(eval_examples).set_custom_predict_fn(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd82c30c5fb4e49b77e955430ee9a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WitWidget(config={'model_type': 'classification', 'label_vocab': [], 'are_sequence_examples': False, 'inferencâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WitWidget(config_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install witwidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with >2.1\n",
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with >0.21.3\n",
    "!pip show tensorflow_model_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with >1.6.0\n",
    "!pip show witwidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing /Users/i854694/.virtualenvs/bmlp2/lib/python3.7/site-packages/witwidget/static -> wit-widget\n",
      "- Validating: \u001b[32mOK\u001b[0m\n",
      "\n",
      "    To initialize this nbextension in the browser every time the notebook (or other app) loads:\n",
      "    \n",
      "          jupyter nbextension enable witwidget --py --sys-prefix\n",
      "    \n",
      "Enabling notebook extension wit-widget/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# may need to run this every time\n",
    "!jupyter nbextension install --py --symlink --sys-prefix witwidget\n",
    "\n",
    "!jupyter nbextension enable witwidget --py --sys-prefix \n",
    "\n",
    "# then refresh browser page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to run this every time\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "  \n",
    "!jupyter nbextension install --py --symlink tensorflow_model_analysis --sys-prefix\n",
    "  \n",
    "!jupyter nbextension enable --py tensorflow_model_analysis --sys-prefix\n",
    "\n",
    "# then refresh browser page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter_nbextensions_configurator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter serverextension list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
